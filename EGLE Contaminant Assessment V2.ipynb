{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8e871a",
   "metadata": {},
   "source": [
    "# Contamination Assessment for Environmental Great Lakes and Energy (EGLE)\n",
    "---\n",
    "## Purpose\n",
    "--- \n",
    "This tool ranks prioritization of site remediation actions for facilities subject to Parts 201 and/or Parts 2013 Based on the proximity to sensitive communities and at-risk populations. It is developed using publicly availably datasets, provided through Michigan's Environmental, Great Lakes, and Energy (EGLE) and integrates querying using geodata sources including geopandas Originally developed in python. \n",
    "\n",
    "### Copyright \n",
    "---\n",
    "Developed by Matthew Jones, Data Scientist, UM MADS Program, Copyright 2022<br>\n",
    "B.Sc. Chemical Engineering BYU | MS Applied Data Science UMich<br>\n",
    "Contact Information: MJones@Envirolytica.com,<br>\n",
    "LinkedIn: https://www.linkedin.com/in/jonesmatthewdavid/\n",
    "\n",
    "### Acceptable Use:\n",
    "---\n",
    "* Educational or non-profit use: Free, but citation required in published work\n",
    "* commerical use: License required, written consent from Envirolytica LLC prior to use\n",
    "\n",
    "### Control Log: \n",
    "---\n",
    "Version 1.0  Released Dec. 2022 by M. Jones, Initial Draft Including Wellhead, Hospital, and School Analysis<br>\n",
    "Version 2.0  Released Sept. 2023 by M. Jones, Updated to include: \n",
    "* Environmental Justice Score\n",
    "* Business and Chemical Risk\n",
    "* Additional Contamination Sites via FOIA \n",
    "* Ranking by District\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39148f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------Beginning of Code----------------------#\n",
    "#--------STEP 1: Import required Packages and set Cut Offs: ---------------------------\n",
    "#This one will load in the FOIA data and merge. \n",
    "import pandas as pd, numpy as np, math, time, datetime as dt, geopandas as gpd, matplotlib.pyplot as plt, collections\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "today = datetime.now()\n",
    "cutoff_date = today.replace(year = today.year - 50) #Future could ask user to define desired age/lookup time\n",
    "retain_depth = 80 #feet\n",
    "well_type = [\"IRRI\", \"TY1PU\", \"TY2PU\", \"TY3PU\", 'HOSHLD']\n",
    "well_status = [\"ACT\", \"UNK\", \"OTH\"]\n",
    "\n",
    "\n",
    "#--------STEP 2: Load Master Datafiles: ---------------------------------------------------------\n",
    "df_contam  = pd.read_csv('Master_contam.csv')\n",
    "df_schools = pd.read_csv('Master_Schools.csv')\n",
    "df_HC      = pd.read_csv('Master_Health_Care.csv')\n",
    "df_WW_ELP  = pd.read_csv('Water_Wells_-_East_Central_Lower_Peninsula.csv')\n",
    "df_WW_NLP  = pd.read_csv('Water_Wells_-_Northern_Lower_Peninsula.csv')\n",
    "df_WW_SCLP = pd.read_csv('Water_Wells_-_South_Central_%26_Southeastern_Michigan.csv')\n",
    "df_WW_SWLP = pd.read_csv('Water_Wells_-_Southwest_Michigan.csv')\n",
    "df_WW_UP   = pd.read_csv('Water_Wells_-_Upper_Peninsula_Master.csv')\n",
    "df_WW_WCLP = pd.read_csv('Water_Wells_-_West_Central_Lower_Peninsula.csv')\n",
    "df_WW_pop  = pd.read_excel('Wellhead Pop Served.xlsx')\n",
    "df_WHPA = gpd.GeoDataFrame.from_file('Wellhead_Protection_Areas.shp')\n",
    "df_WHPA_LL = pd.read_csv('WHPA_point_table.csv')\n",
    "\n",
    "\n",
    "read_time = time.time() - start\n",
    "print('time to load input data ', read_time, \"s\")\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#--------STEP 3: Confirmation Check that all wellhead datafiles have same column structure, Exiting Script if not \n",
    "columns = list(map(lambda x: list(x.columns), [df_WW_ELP,df_WW_NLP, df_WW_SCLP, df_WW_SWLP, df_WW_UP,df_WW_WCLP]))\n",
    "data = pd.DataFrame(columns).fillna(\"\").drop_duplicates(keep=\"first\")\n",
    "if len(data) > 1:\n",
    "    raise Exception (\"\"\"............................\n",
    "    WARNING! Well Data have different columns. Current Script Will Require Modification(s)\n",
    "    .......................\n",
    "    Exiting Script\"\"\")\n",
    "else:  \n",
    "    well_df = pd.concat([df_WW_ELP,df_WW_NLP, df_WW_SCLP, df_WW_SWLP, df_WW_UP,df_WW_WCLP])\n",
    "    \n",
    "    \n",
    "#--------STEP 4: Clean raw data files ---------------------------------------------------------------\n",
    "cols = ['Township', 'Latitude', 'Longitude', 'Facility Name', 'Project Manager', 'EGLE District', 'Full Address', \n",
    "        'Risk Condition', 'Release Status', 'County', 'Senate District' , 'House District','U.S. Congressional District',\n",
    "       'EPA ID','LUST Name', 'City']\n",
    "df_contam = df_contam[df_contam['Release Status'] != \"\\tClosed\"].assign(\n",
    "    **{col: df_contam[col].str.lstrip('\\t').str.rstrip(' ') for col in cols}).astype(\n",
    "    {'Latitude': 'float64', 'Longitude': 'float64'})\n",
    "\n",
    "dict_map = pd.Series(df_contam['EGLE District'].values,index=df_contam['County']).to_dict() #Dict of County: EGLE Dist. \n",
    "df_HC = df_HC.assign(\n",
    "    County=df_HC['County'].astype(str).apply(lambda x: x.lstrip('\\n').lstrip(' ')),\n",
    "    **{        'EGLE District': df_HC['County'].map(dict_map),\n",
    "        'StreetAddress_Used': df_HC['StreetAddress'] + \", \" + df_HC['City'] + \", \" + df_HC['State']})\n",
    "\n",
    "\n",
    "df_schools = df_schools.assign(\n",
    "    **{'EGLE District': df_schools['COUNTY'].map(dict_map),\n",
    "       'Address_Used': df_schools['STREET'] + \", \" + df_schools['CITY'] + \", \" + df_schools['STATE']})\n",
    "\n",
    "\n",
    "well_df = (well_df.merge(\n",
    "    df_WW_pop.assign(WSSN=df_WW_pop['Water Supply Serial Number'].astype('float64')), on='WSSN', how='left')\n",
    "    .assign(\n",
    "        CONST_DATE=lambda x: pd.to_datetime(x['CONST_DATE'].fillna('1900-01-01').str.split(' ').str[0], format='mixed')\n",
    "    )\n",
    ")\n",
    "\n",
    "well_df['EGLE District'] = well_df['COUNTY'].map(dict_map)  # Set the EGLE District for Wells\n",
    "well_df = well_df.loc[\n",
    "    lambda x: x['WELL_TYPE'].isin(well_type) &\n",
    "               x['WEL_STATUS'].isin(well_status) &\n",
    "               (x['WELL_DEPTH'] <= retain_depth) &\n",
    "               (x['CONST_DATE'] <= cutoff_date)\n",
    "]\n",
    "\n",
    "end = time.time()\n",
    "print(\"time to apply initial cleaning\", end-start, \"s\")\n",
    "\n",
    "start = time.time()\n",
    "#--------STEP 5 Read FOIA contaminant Data from FOIA and characterize contaminant by business, prep for merge--------: \n",
    "df_contam_2 = pd.read_csv(\"FOIA_Contam_list.csv\")\n",
    "unique_contaminants = set(df_contam_2[\"Contaminant Class\"])\n",
    "df_contam_2[\"Contaminant ID\"] = df_contam_2[\"Contaminant Class\"].map(contaminants_id)\n",
    "df_contam_2[\"Contaminant Class\"] = df_contam_2[\"Contaminant Class\"].fillna('not specified')#, inplace=True)\n",
    "#Add the contam ID to the Dataframe: \n",
    "df_contam_2[\"loc_class\"] = df_contam_2[\"Location ID\"].astype(str) + df_contam_2[\"Contaminant Class\"]\n",
    "df_contam_2 = df_contam_2.groupby(\"Location ID\").agg({\"Location ID\": \"first\", \"Location Name\":\"first\", \"Address\": \"first\", \"City\": \"first\", \"Zip\": \"first\", \"Latitude\": \"first\", \"Longitude\":\"first\",\"Business Type\": \"first\",\"Contaminant Class\": \"first\", \"loc_class\":\"first\", \"Contaminant ID\": lambda x: list(x)})\n",
    "df_contam_2 = df_contam_2.loc[:, df_contam_2.columns != \"loc_class\"]\n",
    "\n",
    "def count_elements(row):\n",
    "    return len(row)\n",
    "df_contam_2[\"Contaminant Qty\"] = df_contam_2['Contaminant ID'].apply(count_elements)\n",
    "df_contam_2 = df_contam_2.reset_index(drop=True)\n",
    "df_contam_2\n",
    "\n",
    "#Now Weight the Contaminants Present for each site from the FOIA: \n",
    "def weighting_work(column):\n",
    "    weighting_list = []\n",
    "    count_nan =0\n",
    "    for i in range(len(column)):\n",
    "        if len(column[i]) == 1 and (column[i] == 8 or column[i] == 16):\n",
    "            count_nan +=1\n",
    "            group_1 = 0\n",
    "            group_2 = 0 \n",
    "            group_3 = 0\n",
    "        else:\n",
    "            count_0 = column[i].count(0)\n",
    "            count_1 = column[i].count(1)\n",
    "            count_2 = column[i].count(2)\n",
    "            count_3 = column[i].count(3)\n",
    "            count_4 = column[i].count(4)\n",
    "            count_5 = column[i].count(5)\n",
    "            count_6 = column[i].count(6)\n",
    "            count_7 = column[i].count(7)\n",
    "            count_8 = column[i].count(8)\n",
    "            count_9 = column[i].count(9)\n",
    "            count_10 = column[i].count(10)\n",
    "            count_11 = column[i].count(11)\n",
    "            count_12 = column[i].count(12)\n",
    "            count_13 = column[i].count(13)    \n",
    "            count_14 = column[i].count(14)\n",
    "            count_15 = column[i].count(15)\n",
    "            count_16 = column[i].count(16)\n",
    "            count_17 = column[i].count(17)\n",
    "            group_1 = count_16 + count_6 #Chlorinated VOCs and Pesticies\n",
    "            group_2 = count_17 + count_13 + count_8 #PFAS, PBB, PCB, \n",
    "            group_3 = count_11 + count_12  #Petrol, Hydrocarbons\n",
    "            group_4 = count_1 + count_2 + count_3 + count_4 + count_15 #Lead, Mercury, Metals, Dioxins, PAHs\n",
    "            group_5 = count_5 + count_7 + count_9 + count_10+count_14  # Methane, Water Quality, PH, Not Classified \n",
    "            group_6 = count_0 #Not listed\n",
    "        weighting = group_1 + group_2 / 2 + group_3 / 3 + group_4/4 + group_5/5 +group_6/6 \n",
    "        weighting_list.append(weighting)\n",
    "\n",
    "    max_haz = max(weighting_list)\n",
    "    min_haz = min(weighting_list)\n",
    "    weighting_list = [int(((haz - min_haz)/(max_haz-min_haz))*100) for haz in weighting_list]\n",
    "    return(weighting_list)\n",
    "\n",
    "hazard_column = weighting_work(df_contam_2[\"Contaminant ID\"])\n",
    "df_contam_2[\"contaminant hazard\"] = hazard_column\n",
    "\n",
    "#Now Need to Assign the Industry Expected Hazard  based on business type: \n",
    "filter_df = df_contam_2.dropna().loc[df_contam_2['Contaminant Class'] != 'not specified']\n",
    "contaminant_counts = (filter_df.groupby('Business Type')['Contaminant Class']\n",
    "                      .value_counts().groupby(level=0).head(1)\n",
    "                      .reset_index(name='count')\n",
    "                      .sort_values(by=['count'], ascending=[False]))\n",
    "df_contam_2['Business Chemical'] = contaminant_counts['Contaminant Class'].map(contaminants_id)\n",
    "df_contam_2['Business Chemical'] = df_contam_2['Business Chemical'].fillna(0)\n",
    "df_contam_2['Business Chemical'] = df_contam_2['Business Chemical'].astype(int)\n",
    "df_contam_2['Business Chemical'] = df_contam_2['Business Chemical'].apply(lambda x: [x])# \n",
    "df_contam_2['Business Risk']  = weighting_work(df_contam_2['Business Chemical'])\n",
    "\n",
    "business_weight = 0.5\n",
    "contaminant_weight = 0.5\n",
    "df_contam_2['Biz_chem_Risk'] = business_weight*df_contam_2['Business Risk']+contaminant_weight*df_contam_2['contaminant hazard']\n",
    "max_total_risk = df_contam_2['Biz_chem_Risk'].max()\n",
    "min_total_risk = df_contam_2['Biz_chem_Risk'].min()\n",
    "\n",
    "df_contam_2['Biz_chem_Risk'] = (df_contam_2['Biz_chem_Risk'] - min_total_risk) / (max_total_risk - min_total_risk)*100\n",
    "df_contam_2['Biz_chem_Risk'] = df_contam_2['Biz_chem_Risk'].astype(int)\n",
    "df_contam_2[\"detailed address\"] = df_contam_2[\"Address\"] + \", \" + df_contam_2[\"City\"] + \", MI \" + df_contam_2[\"Zip\"]\n",
    "\n",
    "#--------STEP 6: Merge and clean the Contamination sets for Vectorized Analysis-------- \n",
    "contam_df_total = df_contam.merge(df_contam_2, how='outer', left_on='Facility ID', right_on='Location ID')\n",
    "contam_df_total['Latitude'] = np.where(contam_df_total['Latitude_x'].notna(), \n",
    "                                       contam_df_total['Latitude_x'], contam_df_total['Latitude_y'])\n",
    "contam_df_total['Longitude'] = np.where(contam_df_total['Longitude_x'].notna(), \n",
    "                                        contam_df_total['Longitude_x'], contam_df_total['Longitude_y'])\n",
    "contam_df_total['City'] = np.where(contam_df_total['City_x'].notna(), \n",
    "                                   contam_df_total['City_x'], contam_df_total['City_y'])\n",
    "contam_df_total = contam_df_total.dropna(\n",
    "    subset=['Latitude', 'Longitude']).loc[contam_df_total['Risk Condition'] != 'Risk Controlled'].drop(\n",
    "    columns=['Latitude_x', 'Latitude_y', 'City_x', 'City_y', 'Longitude_x', 'Longitude_y', \"U.S. Congressional District\",\n",
    "            \"House District\", \"Senate District\"])\n",
    "\n",
    "contam_df_total.fillna({'Facility Name': contam_df_total['Location Name'],\n",
    "                         'Full Address': contam_df_total['detailed address'],\n",
    "                         'Facility ID': contam_df_total['Location ID'],\n",
    "                         'Risk Condition': 'Risks Not Determined',\n",
    "                         'Project Manager': 'Unknown',\n",
    "                         'Release Status': 'Unknown'}, inplace=True)\n",
    "contam_df_total = contam_df_total[contam_df_total['Release Status'].str.upper() != 'CLOSED']\n",
    "\n",
    "contam_df_total.reset_index(drop=True, inplace=True)\n",
    "\n",
    "end = time.time()\n",
    "print(\"time to add FOIA contaminants and merge datasets\", end-start, \"s\")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#--------STEP 7: Repeat Chemical Assessment on Remaining Columns: --------\n",
    "contam_df_total.head(10)\n",
    "#Now re-assign the business risk for all rows where the business risk is missing.....\n",
    "#set(contam_df_total['contaminant hazard'])\n",
    "hazard_column = weighting_work(contam_df_total[\"Contaminant ID\"])\n",
    "contam_df_total[\"contaminant hazard\"] = hazard_column\n",
    "\n",
    "#Now Need to Assign the Industry Expected Hazard  based on business type: \n",
    "filter_df = contam_df_total.dropna().loc[contam_df_total['Contaminant Class'] != 'not specified']\n",
    "contaminant_counts = (filter_df.groupby('Business Type')['Contaminant Class']\n",
    "                      .value_counts().groupby(level=0).head(1)\n",
    "                      .reset_index(name='count')\n",
    "                      .sort_values(by=['count'], ascending=[False]))\n",
    "contam_df_total['Business Chemical'] = contaminant_counts['Contaminant Class'].map(contaminants_id)\n",
    "contam_df_total['Business Chemical'] = contam_df_total['Business Chemical'].fillna(0)\n",
    "contam_df_total['Business Chemical'] = contam_df_total['Business Chemical'].astype(int)\n",
    "contam_df_total['Business Chemical'] = contam_df_total['Business Chemical'].apply(lambda x: [x])# \n",
    "contam_df_total['Business Risk']  = weighting_work(contam_df_total['Business Chemical'])\n",
    "\n",
    "business_weight = 0.5\n",
    "contaminant_weight = 0.5\n",
    "contam_df_total['Biz_chem_Risk'] = business_weight*contam_df_total['Business Risk']+contaminant_weight*contam_df_total['contaminant hazard']\n",
    "max_total_risk = contam_df_total['Biz_chem_Risk'].max()\n",
    "min_total_risk = contam_df_total['Biz_chem_Risk'].min()\n",
    "\n",
    "contam_df_total['Biz_chem_Risk'] = (contam_df_total['Biz_chem_Risk'] - min_total_risk) / (max_total_risk - min_total_risk)*200\n",
    "contam_df_total['Biz_chem_Risk'] = contam_df_total['Biz_chem_Risk'].astype(int)\n",
    "contam_df_total[\"detailed address\"] = contam_df_total[\"Address\"] + \", \" + contam_df_total[\"City\"] + \", MI \" + contam_df_total[\"Zip\"]\n",
    "contam_df_total['Risk Condition'].fillna('Risks Not Determined', inplace=True)\n",
    "end = time.time()\n",
    "print(\"time to characterize chemical risk and business risk for all facilities\", end-start, \"s\")\n",
    "\n",
    "\n",
    "#--------STEP 8: Identify if the Contam Site is within a Wellhead protection area (Vectorized) ---------------------\n",
    "start = time.time()\n",
    "contam_df_total.reset_index(drop=True, inplace=True)\n",
    "contam_coords = tuple(zip(contam_df_total['Longitude'],contam_df_total['Latitude'])) #tuple of Lat/Long for each contam site\n",
    "WHPA_poly_list ,in_WHPA, point_list = ([] for i in range(3))\n",
    "WHPA = df_WHPA['geometry'] #Lat/Long Coords of each WHPA polygon\n",
    "WHPA_poly_array = np.array(WHPA)\n",
    "contam_coords_array = np.array([Point(coord) for coord in contam_coords])\n",
    "contains_matrix = np.array([poly.contains(point) for poly in WHPA_poly_array for point in contam_coords_array])\n",
    "contains_matrix = contains_matrix.reshape(len(WHPA_poly_array), len(contam_coords_array))\n",
    "is_in_WHPA = np.any(contains_matrix, axis=0)\n",
    "in_WHPA = np.where(is_in_WHPA, \"in WHPA\", \"Not in WHPA\")\n",
    "contam_df_total['IN_WHPA'] = in_WHPA.tolist()\n",
    "end = time.time()\n",
    "print((end-start)/60, \"min to map which Contam Sites are in WHPAs\")\n",
    "\n",
    "well_df, contam_df_total, df_schools, df_HC, df_WHPA, df_WHPA_LL = [\n",
    "    df.reset_index(drop=True) for df in [well_df, contam_df_total, df_schools, df_HC, df_WHPA, df_WHPA_LL]]\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "#--------STEP 9: Determine District If Not Already Specified district--------------------------------------\n",
    "contam_df_total\n",
    "\n",
    "geojson_file_path = 'Counties_(v17a).geojson'\n",
    "gdf = gpd.read_file(geojson_file_path)\n",
    "\n",
    "#Check if \n",
    "contam_coords = tuple(zip(contam_df_total['Longitude'],contam_df_total['Latitude'])) #tuple of Lat/Long for each contam site\n",
    "county_polygons = gdf['geometry'] #Lat/Long Coords of each WHPA polygon\n",
    "county_polygons_list = []\n",
    "county_list = []\n",
    "point_list = [] \n",
    "for i in range(len(county_polygons)):\n",
    "    county_polygons_list.append(county_polygons[i]) #set list of individual polygons\n",
    "for i in range(len(contam_coords)):\n",
    "    point_list.append(Point(contam_coords[i])) #set list of individual contam coords to check\n",
    "\n",
    "#Now Check if the contam site is in the polygon.\n",
    "for j in range(len(point_list)):\n",
    "    point_covered = False\n",
    "    for i in range(len(county_polygons_list)):\n",
    "        if county_polygons_list[i].covers(point_list[j]):\n",
    "            county_list.append(gdf[\"NAME\"][i])\n",
    "            point_covered = True\n",
    "            break\n",
    "    \n",
    "    if not point_covered:\n",
    "        county_list.append(0)\n",
    "contam_df_total[\"County\"] = county_list\n",
    "\n",
    "filtered_df = contam_df_total[contam_df_total['EGLE District'] != 0].dropna(subset=['EGLE District', 'County'])\n",
    "district_to_counties = filtered_df.groupby('EGLE District')['County'].apply(set).to_dict()\n",
    "county_to_district = {county: district for district, counties in district_to_counties.items() for county in counties}\n",
    "del county_to_district[0]\n",
    "contam_df_total['EGLE District'] = contam_df_total['County'].map(county_to_district).fillna(\n",
    "    contam_df_total['EGLE District'])\n",
    "contam_df_total['EGLE District'].fillna('to be determined', inplace=True)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start, \"s to evaluate County\")\n",
    "\n",
    "\n",
    "#-------Step 10: Add in Assessment Risk Factor: \n",
    "risk_mapping = {\n",
    "    'Risks Present and Immediate': 200,\n",
    "    'Risks Present and Require Action in Short-term': 150,\n",
    "    'Risks Present and Require Action in Long-term': 100,\n",
    "    'Risks Not Determined': 100,\n",
    "    'Risks Controlled-Interim': 50,\n",
    "    'Residential Closure (under Section 20101(1)(tt))': 0,\n",
    "    'Contact Lead Division': 100\n",
    "}\n",
    "\n",
    "risk_dict = {item: risk_mapping.get(item, 100) for item in set(contam_df_total[\"Risk Condition\"])}\n",
    "contam_df_total['Assessment Risk'] = contam_df_total['Risk Condition'].map(risk_dict)\n",
    "\n",
    "\n",
    "#-------STEP 11: Calculate the density of schools, wells, and HC sites in vicinity of the contamination site -----------\n",
    "#        to our data dictionary\n",
    "haversine_start = time.time()\n",
    "def get_proximity(item_number):\n",
    "    lat1 = contam_df_total['Latitude'][item_number] * (np.pi / 180)\n",
    "    long1 = contam_df_total['Longitude'][item_number] * (np.pi / 180)\n",
    "\n",
    "    school_lat = df_schools['LATITUDE'] * (np.pi / 180)\n",
    "    school_long = df_schools['LONGITUDE'] * (np.pi / 180)\n",
    "    HC_lat  = df_HC['Latitude']    / (180/np.pi)\n",
    "    HC_long = df_HC['Longitude']   / (180/np.pi)\n",
    "    WHPA_lat  = df_WHPA_LL['POINT_Y']    / (180/np.pi) #latitude\n",
    "    WHPA_long = df_WHPA_LL['POINT_X']   / (180/np.pi) #longitude\n",
    "    Well_lat   = well_df['LATITUDE'] / (180/np.pi)\n",
    "    Well_long  = well_df['LONGITUDE'] / (180/np.pi)\n",
    "    def haversine(lat1, long1, lat2, long2):\n",
    "        dlat = lat2 - lat1\n",
    "        dlong = long2 - long1\n",
    "        a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlong / 2) ** 2\n",
    "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        dist_array = np.sort(3963 * c)  # in miles\n",
    "        return(dist_array)\n",
    "\n",
    "    SC_dist_array = haversine(lat1, long1, school_lat, school_long)\n",
    "    SC_density = np.sum(SC_dist_array <= 1)\n",
    "    SC_worst = np.min(SC_dist_array)\n",
    "\n",
    "    HC_dist_array = haversine(lat1, long1, HC_lat, HC_long)\n",
    "    HC_density = np.sum(HC_dist_array <= 1)\n",
    "    HC_worst = np.min(HC_dist_array)\n",
    "\n",
    "    WHPA_dist_array = haversine(lat1, long1, WHPA_lat, WHPA_long)\n",
    "    WHPA_density = np.sum(WHPA_dist_array <= 1)\n",
    "    WHPA_worst = np.min(WHPA_dist_array)\n",
    "\n",
    "    Well_dist_array = haversine(lat1, long1, Well_lat, Well_long)\n",
    "    Well_density = np.sum(Well_dist_array <= 1)\n",
    "    Well_worst = np.min(Well_dist_array)\n",
    "    return (SC_density,SC_worst, HC_density, HC_worst,Well_density,Well_worst, WHPA_density, WHPA_worst)\n",
    "# Timing\n",
    "\n",
    "myDict = {}\n",
    "contam_df_total.reset_index(drop=True, inplace=True) \n",
    "for i in range(len(contam_df_total)):\n",
    "    #print(i)\n",
    "    proximity = get_proximity(i) \n",
    "    #Building the dictionary that will be output as our master file\n",
    "    myDict[contam_df_total['Facility Name'][i]] = {}\n",
    "    myDict[contam_df_total['Facility Name'][i]]['site ID'] = contam_df_total['Facility ID'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Address'] = contam_df_total['Full Address'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['County']  = contam_df_total['County'][i]    \n",
    "    myDict[contam_df_total['Facility Name'][i]]['school Density'] = proximity[0]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['closest school'] = proximity[1]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Medical Density'] = proximity[2]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Closest HealthCare'] = proximity[3]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Well Density'] = proximity[4]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Closest Well'] = proximity[5]  \n",
    "    myDict[contam_df_total['Facility Name'][i]]['WHPA Density'] = proximity[6]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Closest WHPA'] = proximity[7] \n",
    "    myDict[contam_df_total['Facility Name'][i]]['Contam Lat'] = contam_df_total['Latitude'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Contam Long']=contam_df_total['Longitude'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['in WHPA']=contam_df_total['IN_WHPA'][i]\n",
    "haversine_end = time.time()\n",
    "proximity_time = haversine_end - haversine_start\n",
    "print(\"Time to run Proximity Calculations: \",proximity_time/60, \" minutes\" )\n",
    "\n",
    "\n",
    "#---------STEP 11: Load Env Justice Data--------------: \n",
    "start = time.time()\n",
    "geojson_file_path = 'MiEJScreen_Draft_Data.geojson'\n",
    "gdf = gpd.read_file(geojson_file_path)\n",
    "\n",
    "#Check if \n",
    "contam_coords = tuple(zip(contam_df_total['Longitude'],contam_df_total['Latitude'])) #tuple of Lat/Long for each contam site\n",
    "EJS_polygons = gdf['geometry'] #Lat/Long Coords of each WHPA polygon\n",
    "EJS_polygons_list = []\n",
    "MiEJScore_list = []\n",
    "point_list = [] \n",
    "for i in range(len(EJS_polygons)):\n",
    "    EJS_polygons_list.append(EJS_polygons[i]) #set list of individual polygons\n",
    "for i in range(len(contam_coords)):\n",
    "    point_list.append(Point(contam_coords[i])) #set list of individual contam coords to check\n",
    "\n",
    "#Now Check if the contam site is in the polygon.\n",
    "for j in range(len(point_list)):\n",
    "    point_covered = False\n",
    "    for i in range(len(EJS_polygons_list)):\n",
    "        if EJS_polygons_list[i].covers(point_list[j]):\n",
    "            MiEJScore_list.append(gdf[\"MiEJScreenOverallScore\"][i])\n",
    "            point_covered = True\n",
    "            break\n",
    "    \n",
    "    if not point_covered:\n",
    "        MiEJScore_list.append(0)\n",
    "contam_df_total[\"EJS Sore\"] = MiEJScore_list\n",
    "end = time.time()\n",
    "print(end-start, \"s to evaluate Environmental Justics Score\")\n",
    "\n",
    "\n",
    "#--------STEP 12: Calculate the Total Risk based on proximity to wells, schools, and healthcare ------------------\n",
    "start = time.time()\n",
    "max_sc, max_hc, max_well, max_WH, far_sc, far_hc, far_well, far_WH   = [0,0,0,0,0,0,0,0]\n",
    "close_sc, close_hc, close_well, close_WH = [1000,1000,1000,1000]\n",
    "#First iteration: capture max density of schools, hospital, and wells. \n",
    "for i in (myDict): \n",
    "    if myDict[i]['school Density'] > max_sc: \n",
    "        max_sc = myDict[i]['school Density']\n",
    "    if myDict[i]['Medical Density'] > max_hc:\n",
    "        max_hc = myDict[i]['Medical Density']    \n",
    "    if myDict[i]['Well Density'] > max_well:\n",
    "        max_well = myDict[i]['Well Density']\n",
    "    if myDict[i]['WHPA Density'] > max_WH:\n",
    "        max_WH = myDict[i]['WHPA Density']\n",
    "\n",
    "    if myDict[i]['closest school'] > far_sc: \n",
    "        far_sc = myDict[i]['closest school']\n",
    "    if myDict[i]['Closest HealthCare'] > far_hc:\n",
    "        far_hc = myDict[i]['Closest HealthCare']    \n",
    "    if myDict[i]['Closest Well'] > far_well:\n",
    "        far_well = myDict[i]['Closest Well'] \n",
    "    if myDict[i]['Closest WHPA'] > far_WH:\n",
    "        far_WH = myDict[i]['Closest WHPA']\n",
    "\n",
    "    if myDict[i]['closest school'] < close_sc: \n",
    "        close_sc = myDict[i]['closest school']\n",
    "    if myDict[i]['Closest HealthCare'] < close_hc:\n",
    "        close_sc = myDict[i]['Closest HealthCare']    \n",
    "    if myDict[i]['Closest Well'] < close_well:\n",
    "        close_well = myDict[i]['Closest Well'] \n",
    "    if myDict[i]['Closest WHPA'] < close_WH:\n",
    "        close_WH = myDict[i]['Closest WHPA']\n",
    "        \n",
    "        \n",
    "#Second iteration: assign Risk Weighting based on quantity of sensitive area basis.         \n",
    "for i in (myDict):\n",
    "    SCD = myDict[i]['school Density']\n",
    "    SCC = myDict[i]['closest school']\n",
    "    sc_risk = SCD / max_sc + (1-SCC/far_sc)\n",
    "    myDict[i]['School Risk'] = sc_risk * 100\n",
    "\n",
    "    HCD = myDict[i]['Medical Density']\n",
    "    HCC = myDict[i]['Closest HealthCare']\n",
    "    hc_risk = HCD / max_hc + (1-HCC/far_hc)\n",
    "    myDict[i]['Healthcare Risk'] = hc_risk * 100\n",
    "\n",
    "    if myDict[i]['in WHPA'] == \"in WHPA\":\n",
    "        well_risk = 2\n",
    "        myDict[i]['Well Risk'] = well_risk * 100\n",
    "    else: \n",
    "        WCD = myDict[i]['Well Density']\n",
    "        WCC = myDict[i]['Closest Well']\n",
    "        well_risk = WCD / max_well + (1-WCC/far_well)\n",
    "        myDict[i]['Well Risk'] = well_risk * 100\n",
    "        \n",
    "    #ADD BUSINESS CHEMICAL RISK\n",
    "    myDict[i]['Business Chemical Risk'] = contam_df_total.loc[contam_df_total['Facility ID'] == myDict[i]['site ID'],\n",
    "                                                              'Biz_chem_Risk'].iloc[0]\n",
    "    \n",
    "    myDict[i]['EJS Risk'] = contam_df_total.loc[contam_df_total['Facility ID'] == myDict[i]['site ID'],\n",
    "                                                              'EJS Sore'].iloc[0]\n",
    "    \n",
    "    myDict[i]['Assessment Risk'] = contam_df_total.loc[contam_df_total['Facility ID'] == myDict[i]['site ID'],\n",
    "                                                              'Assessment Risk'].iloc[0]\n",
    "    bc_risk = myDict[i]['Business Chemical Risk']\n",
    "    EJS_risk = myDict[i]['EJS Risk']\n",
    "    a_risk = myDict[i]['Assessment Risk']\n",
    "    total_risk = (sc_risk + hc_risk + well_risk + bc_risk + EJS_risk*2 + a_risk)\n",
    "    myDict[i]['Total Risk'] = total_risk\n",
    "    myDict[i]['Contaminated Site Name'] = i\n",
    "end = time.time()\n",
    "\n",
    "print(end-start, \"s time to Evaluate Total Risk\")\n",
    "\n",
    "\n",
    "#--------STEP 13 load in additional query items for Visualization----------------------------------------\n",
    "for i in range(len(contam_df_total)):\n",
    "    #myDict[Contam_FNM[i]]['Contam Long']=Contam_Long[i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['EGLE Manager'] = contam_df_total['Project Manager'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]][' District'] = contam_df_total['EGLE District'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Risk'] = contam_df_total['Risk Condition'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Release Status'] = contam_df_total['Release Status'][i]\n",
    "    myDict[contam_df_total['Facility Name'][i]]['Regulatory Program'] = contam_df_total['Regulatory Program'][i]\n",
    "\n",
    "    \n",
    "start = time.time()\n",
    "#--------STEP 14: Assign Ranking based on importance by district--------------------------------------\n",
    "#Rank by priority for each district --> ~10s Execution time\n",
    "df_ranking = pd.DataFrame(data=myDict).T\n",
    "df_ranking['Rank_set'] = df_ranking['Total Risk'].rank(ascending=False) #Rank low value = last\n",
    "\n",
    "#Calculate Rank for each district \n",
    "#Need to Fix Rank on District....\n",
    "ranking = []\n",
    "ranking_index = []\n",
    "for district in set(df_ranking[' District']):\n",
    "    data = df_ranking[(df_ranking[' District'] == district)]\n",
    "    rank_list = np.array(data['Total Risk'].rank(ascending=False))\n",
    "    j = 0 \n",
    "    for i in range(len(data)): \n",
    "        if data[' District'][i] == district:\n",
    "            #print(j)\n",
    "            #print(rank_list)\n",
    "            #print(rank_list[j])\n",
    "            j+=1\n",
    "            ranking.append(j)\n",
    "            ranking_index.append(data.index[i])\n",
    "    print(district, \" has been evaluated and ranked\")\n",
    "print(len(ranking),len(df_ranking))\n",
    "#df_ranking['Rank_set2'] = df_ranking['Total Risk'].rank(ascending=False) #Rank low value = last\n",
    "df_district_ranks = pd.DataFrame({\"District Ranking\": ranking, \"Site\":ranking_index})\n",
    "df_ranking = df_ranking.reset_index(drop=True)\n",
    "\n",
    "#Map to the Ranking df\n",
    "output_df = df_ranking.merge(df_district_ranks, left_on=\"Contaminated Site Name\",right_on=\"Site\",  how='left')\n",
    "output_df = output_df.sort_values(by='Total Risk', ascending=False)\n",
    "end = time.time()\n",
    "print(end-start, \"s time to Rank by District\")\n",
    "\n",
    "\n",
    "#--------STEP 15: Assign Ranking based on importance by district--------------------------------------\n",
    "start = time.time()\n",
    "df_well = well_df[['EGLE District', 'WELLID']]\n",
    "df_well.to_excel(\"PBI_well_FileV3.xlsx\")\n",
    "df_schools.to_excel(\"PBI_School_FileV3.xlsx\")\n",
    "df_HC.to_excel(\"PBI_HC_FileV3.xlsx\")\n",
    "output_df.to_excel('EGLE_Contam_Prioritization_V3.xlsx')\n",
    "end = time.time()\n",
    "print(end-start, \"s time to Export Files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980666ce",
   "metadata": {},
   "source": [
    "## Appendix: \n",
    "---\n",
    "### Data Sources:\n",
    "---\n",
    "Links are not guranteed for each site, but were functional at time of development. Example Datafiles available on [Github](https://github.com/MDJonesBYU/EGLE-Contamination-Analysis). <br>\n",
    "[Main Datasource:](https://gis-michigan.opendata.arcgis.com/search?collection=Dataset&q=Public%20Health)<br>\n",
    "[Healthcare Datasource:](https://gis-michigan.opendata.arcgis.com/datasets/Michigan::health-care-1/explore?location=42.964147%2C-85.033721%2C9.82)<br>\n",
    "[Wellhead Datasources:](https://gis-michigan.opendata.arcgis.com/datasets/egle::wellhead-protection-areas/explore?location=42.627007%2C-84.186577%2C8.78)<br>\n",
    "[Well Usage by Popoulation:](https://www.midrinkingwater.org/find_your_public_water_type)<br>\n",
    "[School Data:](https://michigan.maps.arcgis.com/apps/webappviewer/index.html?id=438dc453faf749d786e0c6e8be731cfd)<br>\n",
    "[Contaminant Site Data:](https://www.egle.state.mi.us/RIDE/inventory-of-facilities/facilities)<br>\n",
    "Well Data: <br>&emsp;[UP Wells](https://gis-michigan.opendata.arcgis.com/datasets/egle::water-wells-upper-peninsula/explore?location=46.108409%2C-85.243453%2C12.30&showTable=true)<br>\n",
    "&emsp;LP Wells <br>\n",
    "&emsp;&emsp;[SE Michigan:](https://gis-michigan.opendata.arcgis.com/datasets/egle::water-wells-south-central-southeastern-michigan/explore?location=44.837291%2C-86.135708%2C7.17)<br>\n",
    "&emsp;&emsp;[E Michigan:](https://gis-michigan.opendata.arcgis.com/datasets/egle::water-wells-east-central-lower-peninsula/explore?location=44.753902%2C-86.135708%2C7.00)<br>\n",
    "&emsp;&emsp;[WC Michigan](https://gis-michigan.opendata.arcgis.com/datasets/egle::water-wells-west-central-lower-peninsula/explore?location=44.753902%2C-86.135708%2C7.00)<br>\n",
    "&emsp;&emsp;[N Michigan](https://gis-michigan.opendata.arcgis.com/datasets/egle::water-wells-northern-lower-peninsula/explore)<br>\n",
    "&emsp;&emsp;[SW Michigan](https://gis-michigan.opendata.arcgis.com/datasets/egle::water-wells-southwest-michigan/explore?location=44.868358%2C-86.135708%2C7.64)\n",
    "\n",
    "### Methodology\n",
    "---\n",
    "**Total Risk:**<br> Total Risk Assessed to the Public is based on the following factors:\n",
    "\n",
    "1. The proximity of the contaminated site relative to nearby schools, healthcare facilities, wellhead protection areas, and wells.\n",
    "2. The density of schools, healthcare facilities, wellhead protection areas, and wells in a square mile radius.\n",
    "3. The chemical(s) present at the contaminated site\n",
    "4. The environmental justice score for the surrounding area (as determined by EGLE)\n",
    "5. The assessed urgency of cleanup as termined by EGLE or its designated representatives.\n",
    "**Chemical Risk:**<br> Chemicals information was only provided for ~25% of the 50,000+ contaminated sites. As such business type classifications were used to determine what the 2 most common contaminants were for each business type. Then an assumed contaminant was used for each facility where informaton was not specified.\n",
    "\n",
    "Chemical risk was weighted with Group i / (i) methodology such that the most conscerning group held a weight six times more important than the least concerning group, and was evaluated based on the chemical type, which was prioritized based on whether the chemical was contained in the following groups:\n",
    "\n",
    "* Chlorinated VOCs and Pesticides (Most Concerning)\n",
    "* PFAS, PBB, PCB\n",
    "* Petroleum and Hydrocarbon compounds\n",
    "* Metals including Lead, Mercury, as well as Aromatic hydrocarbons like Dioxin and PAHs\n",
    "* Other chemicals including Methane or items that create concern for Water Quality, PH, or not classified by existing methods.\n",
    "Unknown/Not Listed\n",
    "<br>**District Evaluations:**<br> Subsections were taken to than rank each contaminated site by the severity for each district as well to enable District EGLE employees to priority their contaminant sites appropriately.\n",
    "\n",
    "**Proximity Calculations:** <br> The haversine formula was used to calculate the proximity of different sites to neighboring wells, schools and the like.\n",
    "\n",
    "## Contact Information: \n",
    "MJones@Envirolytica.com<br>\n",
    "www.Envirolytica.com<br>\n",
    "*Envirolytica is a not-for-profit focused on serving government and 501(c)(3) organizations. Reach out if you have interest partnering with us on a project to benefit our community.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65300e13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
